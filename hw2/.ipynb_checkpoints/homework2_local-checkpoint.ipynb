{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSCE 676 :: Data Mining and Analysis :: Texas A&M University :: Fall 2019\n",
    "\n",
    "\n",
    "# Homework 2\n",
    "\n",
    "- **100 points [10% of your final grade]**\n",
    "- **Due Saturday, October 19 by 11:59pm**\n",
    "\n",
    "**Goals of this homework:** There are five objectives of this homework: \n",
    "\n",
    "* Become familiar with Apache Spark and working in a distributed environment in the cloud\n",
    "* Get hands-on experience designing and running a simple MapReduce data transformation job\n",
    "* Get hands-on experience using Spark built-in functions; namely, LDA and PageRank\n",
    "* Design a Pregel algorithm to find tree depth in a network\n",
    "* Understand and implement Trawling algorithm to find user communities\n",
    "\n",
    "*Submission instructions:* You should post your notebook to ecampus (look for the homework 2 assignment there). Name your submission **your-uin_hw2.ipynb**, so for example, my submission would be something like **555001234_hw2.ipynb**. Your notebook should be fully executed when you submit ... so run all the cells for us so we can see the output, then submit that. Follow the AWS guide to create a Hadoop/Spark cluster and create an empty Notebook. Copy all the cells in this notebook to the AWS notebook and continue working on your notebook in AWS. When you are done, download your notebook from AWS (navigate to the location on S3 where your notebook is saved and click download) and submit it to ecampus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the Dataset\n",
    "We will use a dataset of tweets concerning members of the US congress. The data spans almost a year (from October 3rd, 2018 to September 25th, 2019) covering 577 of the members. Any tweet or retweet posted by the 577 members or directed to them by other Twitter users were collected.\n",
    "\n",
    "The data is on S3 in a bucket named s3://us-congress-tweets that you can access. There are 277,744,063 tweets. This is a huge dataset so we will not be working directly on this data all the time. Rather we will work on samples or subsets of this data but in some cases, we will ask you to execute your task on the whole dataset.\n",
    "\n",
    "Below is a summary of all datasets used for this homework:\n",
    "\n",
    "| Dataset                | Location in S3                                      | Description |\n",
    "| :---                   | :---                                                | :---\n",
    "| Congress members       | s3://us-congress-tweets/congress_members.csv        | 577 twitter ids and screen names |\n",
    "| Raw tweets             | s3://us-congress-tweets/raw/\\*.snappy               | the whole json objects of the tweets|\n",
    "| Sample tweets          | s3://us-congress-tweets/congress-sample-10k.json.gz | 10k sample tweets|\n",
    "| Trimmed tweets         | s3://us-congress-tweets/trimmed/\\*.parquet          | selected fields for all tweets|\n",
    "| User hashtags          | s3://us-congress-tweets/user_hashtags.csv           | all pairs of <user, hashtag>|\n",
    "| User replies           | s3://us-congress-tweets/reply_network.csv           | all pairs of <reply_tweet, replied_to_tweet> |\n",
    "| User mentions           | s3://us-congress-tweets/user_mentions.csv           | all pairs of <src_user_id, src_dest_id, frequency> |\n",
    "\n",
    "Let's run some exploration below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # First let's read Twitter ids and screen names of the 577 US congress members\n",
    "\n",
    "# congress_members = spark.read.csv(\"s3://us-congress-tweets/congress_members.csv\", header=True)\n",
    "# congress_members.show()\n",
    "# print(\"Number of congress members tracked:\", congress_members.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `spark.read.json(...)` without schema to load the tweets into a dataframe but this will be slow for two reasons:\n",
    "* First, it will make one pass over the data to build a schema of the content, then a second pass to read the content and parse it to the dataframe. \n",
    "* It will read all the content of the Tweet JSON objects but we only need few fields for a given task.\n",
    "\n",
    "Thus we define our own schema something like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_date_format=\"EEE MMM dd HH:mm:ss ZZZZZ yyyy\"\n",
    "\n",
    "user_schema = StructType([\n",
    "    StructField('created_at',TimestampType(),True),\n",
    "    StructField('followers_count',LongType(),True),\n",
    "    StructField('id',LongType(),True),\n",
    "    StructField('name',StringType(),True),\n",
    "    StructField('screen_name',StringType(),True)\n",
    "])\n",
    "\n",
    "hashtag_schema = ArrayType(StructType([StructField('text',StringType(),True)]))\n",
    "user_mentions_schema = ArrayType(StructType([StructField('id',LongType(),True),\n",
    "                                             StructField('screen_name',StringType(),True)]))\n",
    "entities_schema = StructType([\n",
    "    StructField('hashtags',hashtag_schema,True),\n",
    "    StructField('user_mentions',user_mentions_schema,True)\n",
    "    ])\n",
    "\n",
    "retweeted_status_schema =StructType([        \n",
    "        StructField(\"id\", LongType(), True),\n",
    "        StructField(\"in_reply_to_user_id\", LongType(), True),\n",
    "        StructField(\"in_reply_to_status_id\", LongType(), True),\n",
    "        StructField(\"created_at\", TimestampType(), True),\n",
    "        StructField(\"user\", user_schema)\n",
    "    ])\n",
    "\n",
    "tweet_schema =StructType([\n",
    "        StructField(\"text\", StringType(), True),\n",
    "        StructField(\"id\", LongType(), True),\n",
    "        StructField(\"in_reply_to_user_id\", LongType(), True),\n",
    "        StructField(\"in_reply_to_status_id\", LongType(), True),\n",
    "        StructField(\"created_at\", TimestampType(), True),\n",
    "        StructField(\"user\", user_schema),\n",
    "        StructField(\"entities\", entities_schema),\n",
    "        StructField(\"retweeted_status\", retweeted_status_schema)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to read the tweets with `spark.read.json` passing our own schema as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets = spark.read.option(\"timestampFormat\", twitter_date_format)\\\n",
    "#                    .json('s3://us-congress-tweets/congress-sample-10k.json.gz', tweet_schema)\\\n",
    "#                    .withColumn('user_id',F.col('user.id'))\n",
    "# tweets.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (6 points) Part 1a: Exploratory Data Analysis (Small Scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many unique users and original tweets (i.e. not retweets) are there? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # your code here for unique users\n",
    "# tweets.select(tweets.user.id).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # your code here for original tweets\n",
    "# tweets.filter(tweets.retweeted_status.isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who are the ten most mentioned users in the sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # code and output here\n",
    "# tweets.select(F.col(\"user.screen_name\"),\n",
    "#               F.explode(tweets.entities.user_mentions.screen_name).alias(\"mention\"))\\\n",
    "#       .groupby(\"mention\").count().sort(F.desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the top hashtags used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # code and output here\n",
    "# tweets.select(F.explode(tweets.entities.hashtags.text).alias(\"hashtag\"))\\\n",
    "#       .groupby(\"hashtag\").count().sort(F.desc(\"count\"))\\\n",
    "#       .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4 points) Part 1b: Exploratory Data Analysis (Large Scale)\n",
    "Repeat the above queries but now against the whole dataset defined in the dataframe below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trimmed_files = [x[0] for x in spark.read.csv(\"s3://us-congress-tweets/trimmed/files.txt\").collect()]\n",
    "# tweets_all = spark.read.parquet(*trimmed_files)\n",
    "# tweets_all.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # your code here for unique users\n",
    "# tweets_all.select(tweets_all.user.id).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # your code here for original tweets\n",
    "# tweets_all.filter(tweets_all.retweeted_status.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Top mentioned users code and output here\n",
    "# tweets_all.select(F.col(\"user.screen_name\"),\n",
    "#                   F.explode(tweets_all.entities.user_mentions.screen_name).alias(\"mention\"))\\\n",
    "#       .groupby(\"mention\").count().sort(F.desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Top hashtags code and output here\n",
    "# tweets_all.select(F.explode(tweets_all.entities.hashtags.text).alias(\"hashtag\"))\\\n",
    "#       .groupby(\"hashtag\").count().sort(F.desc(\"count\"))\\\n",
    "#       .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (10 points) Part 2: Textual Analysis (LDA)\n",
    "Using the LDA algorithm provided by the Spark Machine Learning (ML) library, find out the ten most important topics. Use `s3://us-congress-tweets/trimmed/*.parquet` for this task (you can reuse `tweets_all` dataframe from Part1b). \n",
    "\n",
    "You may want to work on a small sample first but report your results on the whole dataset.\n",
    "\n",
    "Hint: for better results aggregate all tweets for a user into a single document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- in_reply_to_user_id: long (nullable = true)\n",
      " |-- in_reply_to_status_id: long (nullable = true)\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- user: struct (nullable = true)\n",
      " |    |-- created_at: timestamp (nullable = true)\n",
      " |    |-- followers_count: long (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- screen_name: string (nullable = true)\n",
      " |-- entities: struct (nullable = true)\n",
      " |    |-- hashtags: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- text: string (nullable = true)\n",
      " |    |-- user_mentions: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |-- screen_name: string (nullable = true)\n",
      " |-- retweeted_status: struct (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- in_reply_to_user_id: long (nullable = true)\n",
      " |    |-- in_reply_to_status_id: long (nullable = true)\n",
      " |    |-- created_at: timestamp (nullable = true)\n",
      " |    |-- user: struct (nullable = true)\n",
      " |    |    |-- created_at: timestamp (nullable = true)\n",
      " |    |    |-- followers_count: long (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- screen_name: string (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.context import SparkContext\n",
    "# from pyspark.sql.session import SparkSession\n",
    "# sc = SparkContext('local')\n",
    "# spark = SparkSession(sc)\n",
    "\n",
    "tweets = spark.read.option(\"timestampFormat\", twitter_date_format)\\\n",
    "                   .json('congress-sample-10k.json.gz', tweet_schema)\\\n",
    "                   .withColumn('user_id',F.col('user.id'))\n",
    "tweets.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # your code here\n",
    "# # Preprocessing - split words, filter out stopwords, group by user ids and aggregate their tweets\n",
    "\n",
    "# # Because processing the whole dataset gives me an error that I can't solve\n",
    "# # even under 8 instances, I sampled 70% of the data\n",
    "# data = tweets_all.sample(False, 0.7)\n",
    "# user_tweet_words = data.select(\"user.id\", F.split(data.text, \"\\s+\").alias(\"text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # StopWordsRemover\n",
    "# from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "# stopWordsRemover = StopWordsRemover(inputCol=\"text\", outputCol=\"filteredText\")\n",
    "# user_tweet_words = stopWordsRemover.transform(user_tweet_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_aggregated_tweets = user_tweet_words.groupby(\"id\")\\\n",
    "#                                          .agg(F.flatten(F.collect_list(\"filteredText\")).alias(\"aggregated_tweets\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "# cv = CountVectorizer(inputCol=\"aggregated_tweets\", outputCol=\"features\", maxDF=8)\n",
    "# cvModel = cv.fit(user_aggregated_tweets)\n",
    "# user_aggregated_tweets = cvModel.transform(user_aggregated_tweets)\n",
    "# user_aggregated_tweets.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.clustering import LDA\n",
    "\n",
    "# lda = LDA(k=10, maxIter=10, optimizer='em')\n",
    "# ldaModel = lda.fit(user_aggregated_tweets)\n",
    "# topics = ldaModel.describeTopics(10)\n",
    "# topics.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each topic, print out 10 words to describe it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # your code here\n",
    "# vocab = cvModel.vocabulary\n",
    "\n",
    "# topic_rows = topics.collect()\n",
    "# for topic in topic_rows:\n",
    "#     terms = []\n",
    "#     for termIndice in topic[1]:\n",
    "#         terms.append(vocab[termIndice].encode('ascii','ignore'))\n",
    "#     print(\"Topic\" + str(topic[0]) + \" : \" + str(terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (10 points) Part 3a: MapRedce\n",
    "In this task, design a MapReduce program in python that reads all the original tweets (no retweets) in the sample tweets (`congress-sample-10k.json.gz`) and if a tweet is a reply to another tweet then output a record of the form <src_id, src_user, dst_id, dst_user>.\n",
    "\n",
    "Create a small cluster (2 or 3 nodes) as per the AWS Guide and then ssh to your cluster and use Hadoop streaming to execute your mapreduce program.\n",
    "\n",
    "Note: the Hadoop streaming jar file can be found at `/usr/lib/hadoop-mapreduce/hadoop-streaming.jar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # your mapper function\n",
    "\n",
    "# #!/usr/bin/env python\n",
    "# import sys\n",
    "# import json\n",
    "\n",
    "# def get_tweet(line):\n",
    "#     try:\n",
    "#         tweet = json.loads(line.strip())\n",
    "#     except:\n",
    "#         tweet = {}\n",
    "\n",
    "#     return tweet\n",
    "\n",
    "# for line in sys.stdin:\n",
    "#     tweet = get_tweet(line)\n",
    "\n",
    "#     # original tweets\n",
    "#     if \"retweeted_status\" not in tweet:\n",
    "#         # reply tweets\n",
    "#         if \"in_reply_to_status_id\" in tweet and tweet[\"in_reply_to_status_id\"] != None:\n",
    "#             print(\"<%s, %s, %s, %s>\" % (\\\n",
    "#                 tweet[\"id\"],\\\n",
    "#                 tweet[\"user\"][\"id\"],\\\n",
    "#                 tweet[\"in_reply_to_status_id\"],\\\n",
    "#                 tweet[\"in_reply_to_user_id\"]\\\n",
    "#             ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your reducer function\n",
    "# mapreduce.job.reduces=0 (0 reducer, map only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # your Hadoop job submission command (copy/paste your command from the terminal)\n",
    "# hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "# -input s3://us-congress-tweets/congress-sample-10k.json.gz \\\n",
    "# -output mapreduce/output \\\n",
    "# -mapper mapper.py \\\n",
    "# -reducer NONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many reply relationships did you get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # code to read job output and count\n",
    "# output = spark.read.csv(\"mapreduce/output\")\n",
    "# output.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5 points) Part 3b: Going Large-Scale with MapReduce\n",
    "\n",
    "Rerun the same MapReduce job above but on the whole dataset (`s3://us-congress-tweets/raw/*.snappy`).\n",
    "All the files under `s3://us-congress-tweets/raw` can be read from the following file:\n",
    "\n",
    "`s3://us-congress-tweets/raw/files.txt`\n",
    "\n",
    "Use shell scripting to parse this file and prepare the input to your MapReduce job as  comma seperated string of all the files. (e.g. your input should be like this `s3://us-congress-tweets/raw/part-00000.snappy,s3://us-congress-tweets/raw/part-00001.snappy,s3://us-congress-tweets/raw/part-00002.snappy,...`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the job logs, how many files did the job operate on? how many input splits were there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Your shell script answer here\n",
    "\n",
    "# #!/bin/bash\n",
    "  \n",
    "# files='s3://us-congress-tweets/raw/files.txt'\n",
    "# inputs=''\n",
    "# file_num=0\n",
    "\n",
    "# while read file; do\n",
    "#     if (( $file_num != 0 ))\n",
    "#     then\n",
    "#         inputs+=','\n",
    "#     fi\n",
    "#     inputs+=$file\n",
    "#     ((file_num+=1))\n",
    "# done < $files\n",
    "\n",
    "# echo $inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# job logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many reply relationships did you get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of reply records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (30 points) Part 4: Graph Analysis\n",
    "In this task, we would like to compute the longest path in *tweet reply* graphs and then perform some statistical calculations on the result. We will use Pregel implementation from GraphFrames for this task. Ignore paths that are longer than 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, construct your tweet reply network using tweet-reply records in this file `s3://us-congress-tweets/reply_network.csv`.\n",
    "From this file, use src_id and dst_id. The dst_id is the id of the tweet being replied to and the src_id is the id of the replying tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import *\n",
    "from graphframes.lib import Pregel\n",
    "sc.setCheckpointDir(\"hdfs:///tmp/graphframes_checkpoint\") # this is needed for any GraphFrames operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # your network construction code here\n",
    "# data = spark.read.csv(\"s3://us-congress-tweets/reply_network.csv\")\n",
    "# data.show()\n",
    "\n",
    "# edges = data.select(\"src_id\", \"dst_id\").cache()\n",
    "# vertices = edges.select(F.col(\"src_id\").alias(\"vertices\")).union(edges.select(\"dst_id\")).distinct()\n",
    "# print(\"# of edges:\", str(edges.count()))\n",
    "# print(\"# of vertices:\", str(vertices.count()))\n",
    "\n",
    "# graph = GraphFrame(vertices, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = spark.createDataFrame([\n",
    "    ['a', 'b'],\n",
    "    ['b', 'd'],\n",
    "    ['b', 'c'],\n",
    "    ['c', 'b'],\n",
    "    ['d', 'c'],\n",
    "    ['d', 'a']\n",
    "], [\"src\", \"dst\"]).cache()\n",
    "edges.show()\n",
    "\n",
    "vertices = edges.select(F.col(\"src_id\").alias(\"vertices\")).union(edges.select(\"dst_id\")).distinct()\n",
    "vertices.show()\n",
    "\n",
    "graph = GraphFrame(vertices, edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the top replied to tweets? (show 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "graph.inDegrees.sort(F.desc(\"inDegree\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many graphs in the reply network? (Hint: use connectedComponents function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "connectedComponents = graph.connectedComponents()\n",
    "connectedComponents.show()\n",
    "connectedComponents.select(\"component\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, design and execute a Pregel program that will calculate the longest paths for all reply graphs in the network. Explain your design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your pregel code here\n",
    "ranks = graph.pregel \\\n",
    "             .setMaxIter(5) \\\n",
    "             .withVertexColumn(\"rank\", \n",
    "                               F.lit(1.0),\n",
    "                               Pregel.msg() * F.lit(0.85) + F.lit(0.15) \n",
    "                              ) \\\n",
    "             .sendMsgToDst(Pregel.src(\"rank\")/Pregel.src(\"outDegree\"))\\\n",
    "             .aggMsgs(F.sum(Pregel.msg())).run()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the average longest path length for all reply graphs in the network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (30 points) Part 5: Community Detection\n",
    "User-hashtag relations have been extracted and saved in the file `s3://us-congress-tweets/user_hashtags.csv`. If a user uses a hashtag there will be a record with the userid and the hashtag.\n",
    "\n",
    "Use the Trawling algorithm discussed in class to find potential user communities in the dataset. (Hint: use FPGrowth in the Spark ML package). Explore different values for the support parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here. Explain all steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List two user communities you think are interesting. Explain why they are reasonable communities.\n",
    "\n",
    "You can use https://twitter.com/intent/user?user_id=? to find out more info about the users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# community 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# community 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What value for support did you choose and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (10 points) Part 6: Personalized PageRank\n",
    "Assume you are given a task to recommend Twitter users for the speaker of the House to engage with.\n",
    "\n",
    "Construct a user-mentions network using relations in `s3://us-congress-tweets/user_mentions.csv`\n",
    "\n",
    "Run Personalized PageRank with source (id=15764644) and find out top accounts to recommend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your network construction code here\n",
    "edges = spark.read.csv(\"s3://us-congress-tweets/user_mentions.csv\", header=True)\n",
    "vertices = spark.read.csv(\"s3://us-congress-tweets/congress_members.csv\", header=True)\n",
    "\n",
    "edges.show()\n",
    "vertices.show()\n",
    "\n",
    "graph = GraphFrame(vertices, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your Personalized PageRank code here\n",
    "graph = GraphFrame(graph.outDegrees, edges)\n",
    "pageranks = graph.pageRank(resetProbability=0.15, maxIter=10, sourceId=\"15764644\")\n",
    "pageranks.show()\n",
    "pageranks.vertices.sort(F.desc(\"pagerank\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 accounts to recommend \n",
    "# You can use https://twitter.com/intent/user?user_id=? to find out more info about the users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Troubleshooting Tips\n",
    "\n",
    "* If you get \"spark not available\" error, this most likely means the Kernel is python and not PySpark. Just change the Kernel to PySpark and it should work.\n",
    "\n",
    "\n",
    "* If your notebook seems stuck (may happen if you force stop a cell), you may need to ssh to your master node and kill the spark application associated with the notebook     \n",
    "    Use `yarn application -list` to find the application id and then `yarn application -kill app-id` to kill it. After that restart your notebook from the browser.\n",
    "\n",
    "\n",
    "* If you like, you may also ssh to the master node and run `pyspark` and execute your code directly in the shell.\n",
    "\n",
    "* If you face difficulties accessing the pages for the jobs for example to see logs and so on then you can open all needed ports when you create the cluster. (e.g. 8088)\n",
    "\n",
    "* If you want to see logs for a MapReduce job from the terminal use the following command:\n",
    "\n",
    "    `yarn logs -applicationId <application_id>`\n",
    "\n",
    "\n",
    "* To kill a MapReduce job use:\n",
    "\n",
    "    `yarn  application -kill <application_id>`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
