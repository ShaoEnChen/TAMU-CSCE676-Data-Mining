{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSCE 676 :: Data Mining and Analysis :: Texas A&M University :: Fall 2019\n",
    "\n",
    "\n",
    "# Homework 2\n",
    "\n",
    "- **100 points [10% of your final grade]**\n",
    "- **Due Saturday, October 19 by 11:59pm**\n",
    "\n",
    "**Goals of this homework:** There are five objectives of this homework: \n",
    "\n",
    "* Become familiar with Apache Spark and working in a distributed environment in the cloud\n",
    "* Get hands-on experience designing and running a simple MapReduce data transformation job\n",
    "* Get hands-on experience using Spark built-in functions; namely, LDA and PageRank\n",
    "* Design a Pregel algorithm to find tree depth in a network\n",
    "* Understand and implement Trawling algorithm to find user communities\n",
    "\n",
    "*Submission instructions:* You should post your notebook to ecampus (look for the homework 2 assignment there). Name your submission **your-uin_hw2.ipynb**, so for example, my submission would be something like **555001234_hw2.ipynb**. Your notebook should be fully executed when you submit ... so run all the cells for us so we can see the output, then submit that. Follow the AWS guide to create a Hadoop/Spark cluster and create an empty Notebook. Copy all the cells in this notebook to the AWS notebook and continue working on your notebook in AWS. When you are done, download your notebook from AWS (navigate to the location on S3 where your notebook is saved and click download) and submit it to ecampus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the Dataset\n",
    "We will use a dataset of tweets concerning members of the US congress. The data spans almost a year (from October 3rd, 2018 to September 25th, 2019) covering 577 of the members. Any tweet or retweet posted by the 577 members or directed to them by other Twitter users were collected.\n",
    "\n",
    "The data is on S3 in a bucket named s3://us-congress-tweets that you can access. There are 277,744,063 tweets. This is a huge dataset so we will not be working directly on this data all the time. Rather we will work on samples or subsets of this data but in some cases, we will ask you to execute your task on the whole dataset.\n",
    "\n",
    "Below is a summary of all datasets used for this homework:\n",
    "\n",
    "| Dataset                | Location in S3                                      | Description |\n",
    "| :---                   | :---                                                | :---\n",
    "| Congress members       | s3://us-congress-tweets/congress_members.csv        | 577 twitter ids and screen names |\n",
    "| Raw tweets             | s3://us-congress-tweets/raw/\\*.snappy               | the whole json objects of the tweets|\n",
    "| Sample tweets          | s3://us-congress-tweets/congress-sample-10k.json.gz | 10k sample tweets|\n",
    "| Trimmed tweets         | s3://us-congress-tweets/trimmed/\\*.parquet          | selected fields for all tweets|\n",
    "| User hashtags          | s3://us-congress-tweets/user_hashtags.csv           | all pairs of <user, hashtag>|\n",
    "| User replies           | s3://us-congress-tweets/reply_network.csv           | all pairs of <reply_tweet, replied_to_tweet> |\n",
    "| User mentions           | s3://us-congress-tweets/user_mentions.csv           | all pairs of <src_user_id, src_dest_id, frequency> |\n",
    "\n",
    "Let's run some exploration below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+\n",
      "|            userid|    screen_name|\n",
      "+------------------+---------------+\n",
      "|         776664410|  RepCartwright|\n",
      "|         240363117|   RepTomMarino|\n",
      "|837722935095930883| RepScottTaylor|\n",
      "|        1069124515|     RepLaMalfa|\n",
      "|818460870573441028|  RepTomGarrett|\n",
      "|         163570705|     repcleaver|\n",
      "|          19739126|      GOPLeader|\n",
      "|          33563161| RepJoseSerrano|\n",
      "|        2861616083|USRepGaryPalmer|\n",
      "|        1074518754| SenatorBaldwin|\n",
      "|         305620929|  Call_Me_Dutch|\n",
      "|         381152398| RepTerriSewell|\n",
      "|         834069080| RepDavidRouzer|\n",
      "|         249787913|  SenatorCarper|\n",
      "|         188019606|        Clyburn|\n",
      "|         217543151|SenatorTimScott|\n",
      "|          39249305| USRepMikeDoyle|\n",
      "|          33537967|   amyklobuchar|\n",
      "|         249410485|  SanfordBishop|\n",
      "|          23124635|    TomColeOK04|\n",
      "+------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "('Number of congress members tracked:', 577)\n"
     ]
    }
   ],
   "source": [
    "# First let's read Twitter ids and screen names of the 577 US congress members\n",
    "\n",
    "congress_members = spark.read.csv(\"s3://us-congress-tweets/congress_members.csv\", header=True)\n",
    "congress_members.show()\n",
    "print(\"Number of congress members tracked:\", congress_members.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `spark.read.json(...)` without schema to load the tweets into a dataframe but this will be slow for two reasons:\n",
    "* First, it will make one pass over the data to build a schema of the content, then a second pass to read the content and parse it to the dataframe. \n",
    "* It will read all the content of the Tweet JSON objects but we only need few fields for a given task.\n",
    "\n",
    "Thus we define our own schema something like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "twitter_date_format=\"EEE MMM dd HH:mm:ss ZZZZZ yyyy\"\n",
    "\n",
    "user_schema = StructType([\n",
    "    StructField('created_at',TimestampType(),True),\n",
    "    StructField('followers_count',LongType(),True),\n",
    "    StructField('id',LongType(),True),\n",
    "    StructField('name',StringType(),True),\n",
    "    StructField('screen_name',StringType(),True)\n",
    "])\n",
    "\n",
    "hashtag_schema = ArrayType(StructType([StructField('text',StringType(),True)]))\n",
    "user_mentions_schema = ArrayType(StructType([StructField('id',LongType(),True),\n",
    "                                             StructField('screen_name',StringType(),True)]))\n",
    "entities_schema = StructType([\n",
    "    StructField('hashtags',hashtag_schema,True),\n",
    "    StructField('user_mentions',user_mentions_schema,True)\n",
    "    ])\n",
    "\n",
    "retweeted_status_schema =StructType([        \n",
    "        StructField(\"id\", LongType(), True),\n",
    "        StructField(\"in_reply_to_user_id\", LongType(), True),\n",
    "        StructField(\"in_reply_to_status_id\", LongType(), True),\n",
    "        StructField(\"created_at\", TimestampType(), True),\n",
    "        StructField(\"user\", user_schema)\n",
    "    ])\n",
    "\n",
    "tweet_schema =StructType([\n",
    "        StructField(\"text\", StringType(), True),\n",
    "        StructField(\"id\", LongType(), True),\n",
    "        StructField(\"in_reply_to_user_id\", LongType(), True),\n",
    "        StructField(\"in_reply_to_status_id\", LongType(), True),\n",
    "        StructField(\"created_at\", TimestampType(), True),\n",
    "        StructField(\"user\", user_schema),\n",
    "        StructField(\"entities\", entities_schema),\n",
    "        StructField(\"retweeted_status\", retweeted_status_schema)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to read the tweets with `spark.read.json` passing our own schema as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- in_reply_to_user_id: long (nullable = true)\n",
      " |-- in_reply_to_status_id: long (nullable = true)\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- user: struct (nullable = true)\n",
      " |    |-- created_at: timestamp (nullable = true)\n",
      " |    |-- followers_count: long (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- screen_name: string (nullable = true)\n",
      " |-- entities: struct (nullable = true)\n",
      " |    |-- hashtags: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- text: string (nullable = true)\n",
      " |    |-- user_mentions: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |-- screen_name: string (nullable = true)\n",
      " |-- retweeted_status: struct (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- in_reply_to_user_id: long (nullable = true)\n",
      " |    |-- in_reply_to_status_id: long (nullable = true)\n",
      " |    |-- created_at: timestamp (nullable = true)\n",
      " |    |-- user: struct (nullable = true)\n",
      " |    |    |-- created_at: timestamp (nullable = true)\n",
      " |    |    |-- followers_count: long (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- screen_name: string (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets = spark.read.option(\"timestampFormat\", twitter_date_format)\\\n",
    "                   .json('s3://us-congress-tweets/congress-sample-10k.json.gz', tweet_schema)\\\n",
    "                   .withColumn('user_id',F.col('user.id'))\n",
    "tweets.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (6 points) Part 1a: Exploratory Data Analysis (Small Scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many unique users and original tweets (i.e. not retweets) are there? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9735"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here for unique users\n",
    "tweets.select(tweets.user.id).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3327"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here for original tweets\n",
    "tweets.filter(tweets.retweeted_status.isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who are the ten most mentioned users in the sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|        mention|count|\n",
      "+---------------+-----+\n",
      "|     SenSchumer|  776|\n",
      "|realDonaldTrump|  751|\n",
      "|  RepAdamSchiff|  739|\n",
      "|     marcorubio|  695|\n",
      "|  SpeakerPelosi|  505|\n",
      "|    NancyPelosi|  368|\n",
      "|       RandPaul|  272|\n",
      "|  ChrisMurphyCT|  223|\n",
      "|  SenGillibrand|  220|\n",
      "|   RepMattGaetz|  220|\n",
      "|     CoryBooker|  214|\n",
      "|  ChuckGrassley|  206|\n",
      "|      JeffFlake|  202|\n",
      "|   SteveScalise|  179|\n",
      "|   amyklobuchar|  176|\n",
      "|      GOPLeader|  164|\n",
      "| RepJerryNadler|  162|\n",
      "|          POTUS|  159|\n",
      "|SenKamalaHarris|  152|\n",
      "| SenJeffMerkley|  141|\n",
      "+---------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# code and output here\n",
    "tweets.select(F.col(\"user.screen_name\"),\n",
    "              F.explode(tweets.entities.user_mentions.screen_name).alias(\"mention\"))\\\n",
    "      .groupby(\"mention\").count().sort(F.desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the top hashtags used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|          hashtag|count|\n",
      "+-----------------+-----+\n",
      "|        Venezuela|  102|\n",
      "|    TrumpShutdown|   42|\n",
      "|     MaduroRegime|   29|\n",
      "|           Maduro|   20|\n",
      "|             MAGA|   20|\n",
      "|      NancyPelosi|   19|\n",
      "|    MuellerReport|   17|\n",
      "|     ForThePeople|   14|\n",
      "|      TrumpResign|   14|\n",
      "|     BuildTheWall|   14|\n",
      "|        Kavanaugh|   14|\n",
      "|     GreenNewDeal|   13|\n",
      "| MyHouseMyAmerica|   12|\n",
      "|        transport|   12|\n",
      "|             Cuba|   10|\n",
      "|        Democrats|   10|\n",
      "|          Florida|   10|\n",
      "|    BrowardCounty|    9|\n",
      "|             EEUU|    8|\n",
      "|MaduroCrimeFamily|    8|\n",
      "+-----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# code and output here\n",
    "tweets.select(F.explode(tweets.entities.hashtags.text).alias(\"hashtag\"))\\\n",
    "      .groupby(\"hashtag\").count().sort(F.desc(\"count\"))\\\n",
    "      .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4 points) Part 1b: Exploratory Data Analysis (Large Scale)\n",
    "Repeat the above queries but now against the whole dataset defined in the dataframe below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- in_reply_to_user_id: long (nullable = true)\n",
      " |-- in_reply_to_status_id: long (nullable = true)\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- user: struct (nullable = true)\n",
      " |    |-- created_at: timestamp (nullable = true)\n",
      " |    |-- followers_count: long (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- screen_name: string (nullable = true)\n",
      " |-- entities: struct (nullable = true)\n",
      " |    |-- hashtags: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- text: string (nullable = true)\n",
      " |    |-- user_mentions: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- id: long (nullable = true)\n",
      " |    |    |    |-- screen_name: string (nullable = true)\n",
      " |-- retweeted_status: struct (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- in_reply_to_user_id: long (nullable = true)\n",
      " |    |-- in_reply_to_status_id: long (nullable = true)\n",
      " |    |-- created_at: timestamp (nullable = true)\n",
      " |    |-- user: struct (nullable = true)\n",
      " |    |    |-- created_at: timestamp (nullable = true)\n",
      " |    |    |-- followers_count: long (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- screen_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trimmed_files = [x[0] for x in spark.read.csv(\"s3://us-congress-tweets/trimmed/files.txt\").collect()]\n",
    "tweets_all = spark.read.parquet(*trimmed_files)\n",
    "tweets_all.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10749403"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here for unique users\n",
    "tweets_all.select(tweets_all.user.id).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96887299"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here for original tweets\n",
    "tweets_all.filter(tweets_all.retweeted_status.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+\n",
      "|        mention|   count|\n",
      "+---------------+--------+\n",
      "|realDonaldTrump|22849070|\n",
      "|LindseyGrahamSC|13420958|\n",
      "|   senatemajldr|12698227|\n",
      "|  RepAdamSchiff|12604975|\n",
      "|     SenSchumer|11879180|\n",
      "|  SpeakerPelosi|11104065|\n",
      "|     marcorubio| 9693611|\n",
      "|     Jim_Jordan| 8450458|\n",
      "|     SenSanders| 6565118|\n",
      "|    RepSwalwell| 5890806|\n",
      "| RepMarkMeadows| 4602132|\n",
      "|          POTUS| 4534031|\n",
      "|    RepCummings| 4210449|\n",
      "| RepJerryNadler| 4195037|\n",
      "|     CoryBooker| 4122082|\n",
      "|      GOPLeader| 4052729|\n",
      "|      SenWarren| 4041194|\n",
      "|   RepMattGaetz| 3865060|\n",
      "|       RandPaul| 3734433|\n",
      "|SenKamalaHarris| 3640507|\n",
      "+---------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top mentioned users code and output here\n",
    "tweets_all.select(F.col(\"user.screen_name\"),\n",
    "                  F.explode(tweets_all.entities.user_mentions.screen_name).alias(\"mention\"))\\\n",
    "      .groupby(\"mention\").count().sort(F.desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+\n",
      "|      hashtag|  count|\n",
      "+-------------+-------+\n",
      "|    Venezuela|1206418|\n",
      "|  MoscowMitch|1105708|\n",
      "|TrumpShutdown| 632069|\n",
      "|         MAGA| 469507|\n",
      "|MuellerReport| 405471|\n",
      "|  NancyPelosi| 359063|\n",
      "| MaduroRegime| 349757|\n",
      "|        Trump| 312651|\n",
      "| BuildTheWall| 311186|\n",
      "| GreenNewDeal| 272289|\n",
      "|     BREAKING| 254382|\n",
      "|    Kavanaugh| 224196|\n",
      "|    Democrats| 211403|\n",
      "| ForThePeople| 207904|\n",
      "|       Maduro| 200107|\n",
      "|         SOTU| 177483|\n",
      "|      Mueller| 170592|\n",
      "|    transport| 160220|\n",
      "| DoWhatWeSaid| 157293|\n",
      "|          HR1| 153986|\n",
      "+-------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top hashtags code and output here\n",
    "tweets_all.select(F.explode(tweets_all.entities.hashtags.text).alias(\"hashtag\"))\\\n",
    "      .groupby(\"hashtag\").count().sort(F.desc(\"count\"))\\\n",
    "      .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (10 points) Part 2: Textual Analysis (LDA)\n",
    "Using the LDA algorithm provided by the Spark Machine Learning (ML) library, find out the ten most important topics. Use `s3://us-congress-tweets/trimmed/*.parquet` for this task (you can reuse `tweets_all` dataframe from Part1b). \n",
    "\n",
    "You may want to work on a small sample first but report your results on the whole dataset.\n",
    "\n",
    "Hint: for better results aggregate all tweets for a user into a single document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# Preprocessing - split words, filter out stopwords, group by user ids and aggregate their tweets\n",
    "\n",
    "# Because processing the whole dataset gives me an error that I can't solve\n",
    "# even under 8 instances, I sampled 70% of the data\n",
    "data = tweets_all.sample(False, 0.8)\n",
    "# data = tweets_all\n",
    "user_tweet_words = data.select(\"user.id\", F.split(data.text, \"\\s+\").alias(\"text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StopWordsRemover\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "stopWordsRemover = StopWordsRemover(inputCol=\"text\", outputCol=\"filteredText\")\n",
    "user_tweet_words = stopWordsRemover.transform(user_tweet_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_tweet_words = user_tweet_words.groupby(\"id\")\\\n",
    "                                   .agg(F.flatten(F.collect_list(\"filteredText\")).alias(\"aggregated_tweets\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+\n",
      "|    id|   aggregated_tweets|            features|\n",
      "+------+--------------------+--------------------+\n",
      "|  3764|[RT, @AOC:, encou...|      (262144,[],[])|\n",
      "|  5556|[RT, @kgahlot:, E...|      (262144,[],[])|\n",
      "| 11938|[@kevburkeie, @pk...|(262144,[239728],...|\n",
      "| 13518|[RT, @jnewland:, ...|      (262144,[],[])|\n",
      "| 26543|[Even, young, man...|      (262144,[],[])|\n",
      "| 35253|[RT, @IsItABikeLa...|(262144,[169427,1...|\n",
      "| 48763|[@SenSchumer, Dea...|      (262144,[],[])|\n",
      "| 60033|[RT, @FictiveCame...|      (262144,[],[])|\n",
      "| 68463|[RT, @RepBrendanB...|      (262144,[],[])|\n",
      "|193283|[@nostridamusontw...|      (262144,[],[])|\n",
      "|601963|[@danforhan, GM, ...|(262144,[202644],...|\n",
      "|660523|[@RepThomasMassie...|      (262144,[],[])|\n",
      "|734203|[RT, @FlipScreen:...|      (262144,[],[])|\n",
      "|747203|[RT, @chrisinsili...|      (262144,[],[])|\n",
      "|763684|[said, say, again...|      (262144,[],[])|\n",
      "|781066|[RT, @SenGaryPete...|      (262144,[],[])|\n",
      "|781154|[Listen:, recycle...|      (262144,[],[])|\n",
      "|794532|[Ahead, tax, day,...|      (262144,[],[])|\n",
      "|806000|[@IAmChrisCrespo,...|      (262144,[],[])|\n",
      "|814261|[RT, @KatieHill4C...|      (262144,[],[])|\n",
      "+------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "# The maximum and minimum occurrence can be further tuned to get better representative topics\n",
    "cv = CountVectorizer(inputCol=\"aggregated_tweets\", outputCol=\"features\", maxDF=50, minDF=5)\n",
    "cvModel = cv.fit(user_tweet_words)\n",
    "user_tweet_words = cvModel.transform(user_tweet_words)\n",
    "user_tweet_words.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|topic|         termIndices|         termWeights|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|[2, 3, 4, 5, 6, 8...|[0.00870556566196...|\n",
      "|    1|[0, 1, 12, 13, 30...|[0.01609675647858...|\n",
      "|    2|[64, 69, 81, 105,...|[0.00132209943962...|\n",
      "|    3|[10, 24, 28, 29, ...|[0.00395606202193...|\n",
      "|    4|[50, 59, 62, 72, ...|[0.00125446835767...|\n",
      "|    5|[42, 66, 157, 160...|[0.00143052106111...|\n",
      "|    6|[22, 27, 35, 44, ...|[0.00294871844130...|\n",
      "|    7|[7, 32, 58, 60, 7...|[0.00504062626150...|\n",
      "|    8|[43, 49, 75, 78, ...|[0.00151722481547...|\n",
      "|    9|[14, 23, 41, 63, ...|[0.00290916015885...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "lda = LDA(k=10, optimizer='em')\n",
    "ldaModel = lda.fit(user_tweet_words)\n",
    "topics = ldaModel.describeTopics(10)\n",
    "topics.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each topic, print out 10 words to describe it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic0 : ['@Rutherford_Inst', '@WEXWatchdog', '@CCHR', '@johnalexwood', '@NMPoliticsnet', '@haussamen', '@soljourno', '@nmdoh', 'Hiring:', 'GOPArkansas']\n",
      "Topic1 : ['@AngelCIraq214', '@news_store_com', '@Jstaskin:', '@lowkell', 'HARIHAR', '@Scooterocket', '@screamguitarman:', '\"Ratcliffe', '@franceonu', '@laborers435']\n",
      "Topic2 : ['@BreakingNews', '@SonOfJmkWalkow:', '@GotTeam:', 'Guidestones', 'Radio', 'Tadler', '#comobilit', '@Cr8rBoi', \"Paddlin'\", '@andrewscheer']\n",
      "Topic3 : ['2019-02-20,', '2018-12-20,', 'load:', '#manandvan', '@Padres', '@MLBStats', '@LMErdosSCR_APS', '@Republicist1:', '\"Hurd', '(R-San']\n",
      "Topic4 : ['\"Kenny', 'Gangstalking', '#TweetTheMuellerReport', \"(R-Coppell)'s'\", '@Teamsters', '2018-11-15,', '2018-12-13,', '#OFFICE', '2018-12-22,', '#LOCAL']\n",
      "Topic5 : ['@DomainLandlords:', '#GOPChairwoman', '@ShayEvaSatchel', '@bent_alsaudia10', '@eqibeat:', '@BellaLettie3', '@sandrasmithfox', '@cherrynchester', '#KamalaForThePeople', '@jphoganorg:']\n",
      "Topic6 : ['Volg', 'online!', '@HRW', 'Lawanna', '#asksteveforjobs', '\"Louie', \"(R-Tyler)'s'\", '#TXpols', '@gopmillennials', '#PApols']\n",
      "Topic7 : ['@eltonofficial', '@KleynTrucks', '@uriminzok', '@xijingping', '@GinniRometty', '@ispotify1:', '#buildthedamnwallnow', '\"Conaway', '@TheRealRazielah:', '$8.55']\n",
      "Topic8 : ['@StrengthINumber:', '@judgejeffbrown', '@hairlossclinic1', '@glopol_analysis', '.@Sororita', '@PVallum', '@FCriticalThink', '@BenStanton77', '@BettysLove:', '@SiSePuede_2020:']\n",
      "Topic9 : ['\"rebuild', 'trust\"...\"can', '@leadelaria:', '@RitaWilson', \"(R-Weatherford)'s'\", '@LindseyKevitch', '@wqbelle:', '@W_ProphetBalik:', '@USSupreme_Court', '$3,300,000']\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "vocab = cvModel.vocabulary\n",
    "\n",
    "topic_rows = topics.collect()\n",
    "for topic in topic_rows:\n",
    "    terms = []\n",
    "    for termIndice in topic[1]:\n",
    "        terms.append(vocab[termIndice].encode('ascii','ignore'))\n",
    "    print(\"Topic\" + str(topic[0]) + \" : \" + str(terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (10 points) Part 3a: MapRedce\n",
    "In this task, design a MapReduce program in python that reads all the original tweets (no retweets) in the sample tweets (`congress-sample-10k.json.gz`) and if a tweet is a reply to another tweet then output a record of the form <src_id, src_user, dst_id, dst_user>.\n",
    "\n",
    "Create a small cluster (2 or 3 nodes) as per the AWS Guide and then ssh to your cluster and use Hadoop streaming to execute your mapreduce program.\n",
    "\n",
    "Note: the Hadoop streaming jar file can be found at `/usr/lib/hadoop-mapreduce/hadoop-streaming.jar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your mapper function\n",
    "\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import json\n",
    "\n",
    "def get_tweet(line):\n",
    "    try:\n",
    "        tweet = json.loads(line.strip())\n",
    "    except:\n",
    "        tweet = {}\n",
    "\n",
    "    return tweet\n",
    "\n",
    "for line in sys.stdin:\n",
    "    tweet = get_tweet(line)\n",
    "\n",
    "    # original tweets\n",
    "    if \"retweeted_status\" not in tweet:\n",
    "        # reply tweets\n",
    "        if \"in_reply_to_status_id\" in tweet and tweet[\"in_reply_to_status_id\"] != None:\n",
    "            print(\"<%s, %s, %s, %s>\" % (\\\n",
    "                tweet[\"id\"],\\\n",
    "                tweet[\"user\"][\"id\"],\\\n",
    "                tweet[\"in_reply_to_status_id\"],\\\n",
    "                tweet[\"in_reply_to_user_id\"]\\\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your reducer function\n",
    "mapreduce.job.reduces=0 (0 reducer, map only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your Hadoop job submission command (copy/paste your command from the terminal)\n",
    "hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
    "-input s3://us-congress-tweets/congress-sample-10k.json.gz\n",
    "-output mapreduce/output2\n",
    "-mapper mapper.py\n",
    "-reducer NONE\n",
    "-file mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Job job_1572548302598_0005 completed successfully\n",
    "19/10/31 23:42:12 INFO mapreduce.Job: Counters: 35\n",
    "\tFile System Counters\n",
    "\t\tFILE: Number of bytes read=0\n",
    "\t\tFILE: Number of bytes written=172914\n",
    "\t\tFILE: Number of read operations=0\n",
    "\t\tFILE: Number of large read operations=0\n",
    "\t\tFILE: Number of write operations=0\n",
    "\t\tHDFS: Number of bytes read=103\n",
    "\t\tHDFS: Number of bytes written=163816\n",
    "\t\tHDFS: Number of read operations=4\n",
    "\t\tHDFS: Number of large read operations=0\n",
    "\t\tHDFS: Number of write operations=2\n",
    "\t\tS3: Number of bytes read=12616508\n",
    "\t\tS3: Number of bytes written=0\n",
    "\t\tS3: Number of read operations=0\n",
    "\t\tS3: Number of large read operations=0\n",
    "\t\tS3: Number of write operations=0\n",
    "\tJob Counters \n",
    "\t\tLaunched map tasks=1\n",
    "\t\tData-local map tasks=1\n",
    "\t\tTotal time spent by all maps in occupied slots (ms)=577632\n",
    "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
    "\t\tTotal time spent by all map tasks (ms)=6017\n",
    "\t\tTotal vcore-milliseconds taken by all map tasks=6017\n",
    "\t\tTotal megabyte-milliseconds taken by all map tasks=18484224\n",
    "\tMap-Reduce Framework\n",
    "\t\tMap input records=10000\n",
    "\t\tMap output records=2311\n",
    "\t\tInput split bytes=103\n",
    "\t\tSpilled Records=0\n",
    "\t\tFailed Shuffles=0\n",
    "\t\tMerged Map outputs=0\n",
    "\t\tGC time elapsed (ms)=105\n",
    "\t\tCPU time spent (ms)=5650\n",
    "\t\tPhysical memory (bytes) snapshot=526106624\n",
    "\t\tVirtual memory (bytes) snapshot=4663517184\n",
    "\t\tTotal committed heap usage (bytes)=449839104\n",
    "\tFile Input Format Counters \n",
    "\t\tBytes Read=12616508\n",
    "\tFile Output Format Counters \n",
    "\t\tBytes Written=163816\n",
    "19/10/31 23:42:12 INFO streaming.StreamJob: Output directory: mapreduce/output2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many reply relationships did you get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2311"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code to read job output and count\n",
    "output = spark.read.csv(\"mapreduce/output2\")\n",
    "output.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5 points) Part 3b: Going Large-Scale with MapReduce\n",
    "\n",
    "Rerun the same MapReduce job above but on the whole dataset (`s3://us-congress-tweets/raw/*.snappy`).\n",
    "All the files under `s3://us-congress-tweets/raw` can be read from the following file:\n",
    "\n",
    "`s3://us-congress-tweets/raw/files.txt`\n",
    "\n",
    "Use shell scripting to parse this file and prepare the input to your MapReduce job as  comma seperated string of all the files. (e.g. your input should be like this `s3://us-congress-tweets/raw/part-00000.snappy,s3://us-congress-tweets/raw/part-00001.snappy,s3://us-congress-tweets/raw/part-00002.snappy,...`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the job logs, how many files did the job operate on? how many input splits were there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 _c0|\n",
      "+--------------------+\n",
      "|s3://us-congress-...|\n",
      "|s3://us-congress-...|\n",
      "|s3://us-congress-...|\n",
      "|s3://us-congress-...|\n",
      "|s3://us-congress-...|\n",
      "|s3://us-congress-...|\n",
      "|s3://us-congress-...|\n",
      "|s3://us-congress-...|\n",
      "|s3://us-congress-...|\n",
      "|s3://us-congress-...|\n",
      "|s3://us-congress-...|\n",
      "|s3://us-congress-...|\n",
      "|s3://us-congress-...|\n",
      "|s3://us-congress-...|\n",
      "|s3://us-congress-...|\n",
      "|s3://us-congress-...|\n",
      "|s3://us-congress-...|\n",
      "|s3://us-congress-...|\n",
      "|s3://us-congress-...|\n",
      "|s3://us-congress-...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your answer here\n",
    "mapreduce_files = spark.read.csv(\"s3://us-congress-tweets/raw/files.txt\")\n",
    "mapreduce_files.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
    "-mapper mapper.py\n",
    "-file mapper.py\n",
    "-reducer NONE\n",
    "$(./prep_input.sh)\n",
    "-output mapreduce/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep_input.sh\n",
    "#!/bin/sh\n",
    "\n",
    "# ran \"aws s3 cp s3://us-congress-tweets/raw/files.txt .\" to get file list to local\n",
    "files='./files.txt'\n",
    "# inputs=''\n",
    "# file_num=0\n",
    "\n",
    "IFS=$'\\n'\n",
    "while read file; do\n",
    "#    if (( $file_num != 0 ))\n",
    "#    then\n",
    "#        inputs+=','\n",
    "#    fi\n",
    "#    inputs+=$file\n",
    "#    ((file_num+=1))\n",
    "    echo '-input' $file\n",
    "done < $files\n",
    "\n",
    "# echo $inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make inputs comma-separated\n",
    "#!/bin/sh\n",
    "\n",
    "files='./files.txt'\n",
    "inputs=''\n",
    "file_num=0\n",
    "\n",
    "while read file; do\n",
    "   if (( $file_num != 0 ))\n",
    "   then\n",
    "       inputs+=','\n",
    "   fi\n",
    "   inputs+=$file\n",
    "   ((file_num+=1))\n",
    "done < $files\n",
    "\n",
    "echo $inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# job logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many reply relationships did you get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85493943\n"
     ]
    }
   ],
   "source": [
    "# Number of reply records\n",
    "output = spark.read.csv(\"mapreduce/output\")\n",
    "output.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (30 points) Part 4: Graph Analysis\n",
    "In this task, we would like to compute the longest path in *tweet reply* graphs and then perform some statistical calculations on the result. We will use Pregel implementation from GraphFrames for this task. Ignore paths that are longer than 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, construct your tweet reply network using tweet-reply records in this file `s3://us-congress-tweets/reply_network.csv`.\n",
    "From this file, use src_id and dst_id. The dst_id is the id of the tweet being replied to and the src_id is the id of the replying tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import *\n",
    "from graphframes.lib import Pregel\n",
    "sc.setCheckpointDir(\"hdfs:///tmp/graphframes_checkpoint\") # this is needed for any GraphFrames operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+------------------+\n",
      "|             src_id|           src_user|             dst_id|          dst_user|\n",
      "+-------------------+-------------------+-------------------+------------------+\n",
      "|1047536930651611137| 787776811371487233|1047536497052844032|787776811371487233|\n",
      "|1047537000385929216|1018859733736779777|1047529220065447936|         970207298|\n",
      "|1047537019386093568|          461479285|1047490615829827585|          19821744|\n",
      "|1047537318461026304|         2681991511|1047504406646808576|          39344374|\n",
      "|1047537380981465088| 831305062785937408|1047517885260750853|           1917731|\n",
      "|1047537448295813120| 924843693047037952|1047465256023445504|          25073877|\n",
      "|1047537571146944512|         4268534475|1047504990011572229|           7301572|\n",
      "|1047537751049064448|           20344298|1047534440757579777|980676000152514560|\n",
      "|1047538150959136772|          532438672|1047529230203047937|          19417492|\n",
      "|1047538603008696322|1047529154772717568|1047132430098927617|          29442313|\n",
      "|1047538630573678592|          734951545|1047508807675531264|         262756641|\n",
      "|1047538803005636608| 786347704926412800|1047537257727504384|        2166380495|\n",
      "|1047539317055377410| 871344702825660416|1047538522289258499|           1917731|\n",
      "|1047539502946967553|1046774554427895808|1047519678749442048|          17494010|\n",
      "|1047539625361887234| 730767276006244352|1047516105567363073|          10615232|\n",
      "|1047539874662928385|         4371722352|1047531068738293760|         164679813|\n",
      "|1047540275818651649|         3120679844|1047503978534264834|          25429371|\n",
      "|1047540523312078849|           16180876|1046876489113907201|        1249982359|\n",
      "|1047540591695777792|         1840430268|1047539231181213696|         101540699|\n",
      "|1047540640588996609|          138844863|1047540255409229825|          28485602|\n",
      "+-------------------+-------------------+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your network construction code here\n",
    "data = spark.read.csv(\"s3://us-congress-tweets/reply_network.csv\", header=True)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|                src|                dst|\n",
      "+-------------------+-------------------+\n",
      "|1047536930651611137|1047536497052844032|\n",
      "|1047537000385929216|1047529220065447936|\n",
      "|1047537019386093568|1047490615829827585|\n",
      "|1047537318461026304|1047504406646808576|\n",
      "|1047537380981465088|1047517885260750853|\n",
      "|1047537448295813120|1047465256023445504|\n",
      "|1047537571146944512|1047504990011572229|\n",
      "|1047537751049064448|1047534440757579777|\n",
      "|1047538150959136772|1047529230203047937|\n",
      "|1047538603008696322|1047132430098927617|\n",
      "|1047538630573678592|1047508807675531264|\n",
      "|1047538803005636608|1047537257727504384|\n",
      "|1047539317055377410|1047538522289258499|\n",
      "|1047539502946967553|1047519678749442048|\n",
      "|1047539625361887234|1047516105567363073|\n",
      "|1047539874662928385|1047531068738293760|\n",
      "|1047540275818651649|1047503978534264834|\n",
      "|1047540523312078849|1046876489113907201|\n",
      "|1047540591695777792|1047539231181213696|\n",
      "|1047540640588996609|1047540255409229825|\n",
      "+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------------+\n",
      "|                 id|\n",
      "+-------------------+\n",
      "|1047544248722022400|\n",
      "|1047547544673243137|\n",
      "|1047594252002320387|\n",
      "|1047596366317461505|\n",
      "|1047634620269957120|\n",
      "|1047682015095480320|\n",
      "|1047768349831192576|\n",
      "|1047790026434732032|\n",
      "|1047804074240835584|\n",
      "|1047816594691428352|\n",
      "|1047839129088741378|\n",
      "|1047888345953656833|\n",
      "|1047908336690184192|\n",
      "|1047912187707240448|\n",
      "|1047920343631900672|\n",
      "|1047929185266802695|\n",
      "|1047987541876662272|\n",
      "|1047991526289264640|\n",
      "|1047993769952534533|\n",
      "|1048005033386622976|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "('# of edges:', '71644975')\n",
      "('# of vertices:', '76510188')\n"
     ]
    }
   ],
   "source": [
    "edges = data.select(F.col(\"src_id\").alias(\"src\"), F.col(\"dst_id\").alias(\"dst\")).cache()\n",
    "vertices = edges.select(F.col(\"src\").alias(\"id\")).union(edges.select(\"dst\")).distinct()\n",
    "\n",
    "edges.show()\n",
    "vertices.show()\n",
    "print(\"# of edges:\", str(edges.count()))\n",
    "print(\"# of vertices:\", str(vertices.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = GraphFrame(vertices, edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the top replied to tweets? (show 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+\n",
      "|                 id|inDegree|\n",
      "+-------------------+--------+\n",
      "|1157787985041088513|   94351|\n",
      "|1048314564826292227|   76396|\n",
      "|1111289977143545856|   68172|\n",
      "|1155949756792725510|   65241|\n",
      "|1137060666223878144|   57764|\n",
      "|1062461047892787204|   53767|\n",
      "|1158036816089497601|   50059|\n",
      "|1144730911889428480|   45905|\n",
      "|1155949605147648006|   44810|\n",
      "|1098312693436596226|   41836|\n",
      "|1150408691713265665|   41825|\n",
      "|1129831615952236546|   41556|\n",
      "|1150859069084905472|   39020|\n",
      "|1144078421670150144|   38687|\n",
      "|1155132215208161281|   38572|\n",
      "|1088141172638400512|   37102|\n",
      "|1155469517092470784|   36961|\n",
      "|1168938037071482881|   36728|\n",
      "|1154161356171599877|   36552|\n",
      "|1131740851909083137|   36386|\n",
      "+-------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "graph.inDegrees.sort(F.desc(\"inDegree\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many graphs in the reply network? (Hint: use connectedComponents function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+\n",
      "|                 id|component|\n",
      "+-------------------+---------+\n",
      "|1018902211320041474|       26|\n",
      "|1019644844275388417|       29|\n",
      "|1047565848481681408|      474|\n",
      "|1047636184011169792|      917|\n",
      "|1047777710309892096|     1226|\n",
      "|1047790518422396928|     1697|\n",
      "|1047818674684485633|      846|\n",
      "|1047844212517957632|     1913|\n",
      "|1047856196042932225|     2040|\n",
      "|1047872894808854528|     2214|\n",
      "|1047874785362042881|      517|\n",
      "|1047888044609753088|     2449|\n",
      "|1047892115215343616|     2135|\n",
      "|1047893334587584512|     2436|\n",
      "|1047926063446339584|     1650|\n",
      "|1047935603764023298|     3091|\n",
      "|1047963278176149504|     2460|\n",
      "|1047983667933564928|     3764|\n",
      "|1048062759928250368|     2826|\n",
      "|1048154635733803008|     3325|\n",
      "+-------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "connectedComponents = graph.connectedComponents()\n",
    "connectedComponents.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4865213"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connectedComponents.select(\"component\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, design and execute a Pregel program that will calculate the longest paths for all reply graphs in the network. Explain your design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your pregel code here\n",
    "# Since we want to ignore paths longer than 20, we limit # of iterations to 20\n",
    "# We set the initial path length of each vertex to 0\n",
    "# and increase path length by 1 each time a vertex receives a messeage\n",
    "\n",
    "longestPath = graph.pregel\\\n",
    "             .setMaxIter(20)\\\n",
    "             .withVertexColumn(\"longestPath\",\n",
    "                               F.lit(0),\n",
    "                               F.coalesce(Pregel.msg(), F.col(\"longestPath\"))\n",
    "                              )\\\n",
    "             .sendMsgToDst(Pregel.src(\"longestPath\") + 1)\\\n",
    "             .aggMsgs(F.max(Pregel.msg())).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+\n",
      "|                 id|longestPath|\n",
      "+-------------------+-----------+\n",
      "|1000097150066348033|          1|\n",
      "|1000610279376457728|          1|\n",
      "|1004461558666125314|          1|\n",
      "|1004552938445066240|          1|\n",
      "|1005074779672629249|          3|\n",
      "|1005096402660274176|          1|\n",
      "|1006976555032219649|          1|\n",
      "|1007196442971267074|          1|\n",
      "|1007385497633779712|          1|\n",
      "|1007520380263780352|          1|\n",
      "|1007653218682318849|          6|\n",
      "|1007755524505587712|          1|\n",
      "|1009465122618839042|          1|\n",
      "|1010679848828669952|          1|\n",
      "|1011750104666001408|          1|\n",
      "|1011825432289796096|          1|\n",
      "|1012416740448448513|          1|\n",
      "|1014581448354000903|          1|\n",
      "|1015396907689103360|          1|\n",
      "|1015557994271117314|          1|\n",
      "+-------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "longestPath.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the average longest path length for all reply graphs in the network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+---------+\n",
      "|                 id|longestPath|component|\n",
      "+-------------------+-----------+---------+\n",
      "|1000097150066348033|          1|        0|\n",
      "|1000610279376457728|          1|        1|\n",
      "|1004461558666125314|          1|        2|\n",
      "|1004552938445066240|          1|        3|\n",
      "|1005074779672629249|          3|        4|\n",
      "|1005096402660274176|          1|        5|\n",
      "|1006976555032219649|          1|        6|\n",
      "|1007196442971267074|          1|        7|\n",
      "|1007385497633779712|          1|        8|\n",
      "|1007520380263780352|          1|        9|\n",
      "|1007653218682318849|          6|       10|\n",
      "|1007755524505587712|          1|       11|\n",
      "|1009465122618839042|          1|       12|\n",
      "|1010679848828669952|          1|       13|\n",
      "|1011750104666001408|          1|       14|\n",
      "|1011825432289796096|          1|       15|\n",
      "|1012416740448448513|          1|       16|\n",
      "|1014581448354000903|          1|       17|\n",
      "|1015396907689103360|          1|       18|\n",
      "|1015557994271117314|          1|       19|\n",
      "+-------------------+-----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "longestPath = longestPath.join(connectedComponents, [\"id\"])\n",
    "longestPath.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+\n",
      "|  component|max(longestPath)|\n",
      "+-----------+----------------+\n",
      "|51539608871|               2|\n",
      "|51539609457|               1|\n",
      "|51539609521|               2|\n",
      "|51539609867|               1|\n",
      "| 8589936972|              16|\n",
      "|51539610594|               4|\n",
      "|      62612|               8|\n",
      "|51539611367|               1|\n",
      "|       3764|               8|\n",
      "|51539612216|               2|\n",
      "| 8589939696|               1|\n",
      "|51539612235|               1|\n",
      "|51539612488|               1|\n",
      "|51539612954|               4|\n",
      "|34359744093|              14|\n",
      "|       7747|              16|\n",
      "|51539613657|               1|\n",
      "|51539615099|               1|\n",
      "| 8589942632|               3|\n",
      "|      15437|              20|\n",
      "+-----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+----------------+\n",
      "| component|max(longestPath)|\n",
      "+----------+----------------+\n",
      "|8590015831|              20|\n",
      "|    149406|              20|\n",
      "|     81798|              20|\n",
      "|8589937624|              20|\n",
      "|     81875|              20|\n",
      "|     10123|              20|\n",
      "|     94971|              20|\n",
      "|     48541|              20|\n",
      "|     33874|              20|\n",
      "|     34386|              20|\n",
      "|    116469|              20|\n",
      "|    167369|              20|\n",
      "|    121286|              20|\n",
      "|8589936081|              20|\n",
      "|    121825|              20|\n",
      "|      6252|              20|\n",
      "|    125812|              20|\n",
      "|8589981871|              20|\n",
      "|    154705|              20|\n",
      "|      3324|              20|\n",
      "+----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "componentLongestPath = longestPath.groupby(\"component\").max(\"longestPath\")\n",
    "componentLongestPath.show()\n",
    "componentLongestPath.sort(F.desc(\"max(longestPath)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|avg(max(longestPath))|\n",
      "+---------------------+\n",
      "|   1.5533420633382342|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "componentLongestPath.agg({\"max(longestPath)\" : \"avg\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (30 points) Part 5: Community Detection\n",
    "User-hashtag relations have been extracted and saved in the file `s3://us-congress-tweets/user_hashtags.csv`. If a user uses a hashtag there will be a record with the userid and the hashtag.\n",
    "\n",
    "Use the Trawling algorithm discussed in class to find potential user communities in the dataset. (Hint: use FPGrowth in the Spark ML package). Explore different values for the support parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|                 id|            hashtags|\n",
      "+-------------------+--------------------+\n",
      "|1000056825981603841| [believeallscience]|\n",
      "|         1000074318|             [Doral]|\n",
      "|1000341730582061056|[GIVEAWAY, Summer...|\n",
      "|         1000356566|            [clowns]|\n",
      "|1000365529406853121|            [Careem]|\n",
      "|1000460805366796288|[Management, Lead...|\n",
      "|1000463993499156481|[MAGA, FreeSpeech...|\n",
      "|1000480351905632258|[NancyPelosi, TAV...|\n",
      "|          100048155|    [HungertoHealth]|\n",
      "|1000785956298018817|[crowdstrike, BRE...|\n",
      "|1000870665577152513|[StarWarsGalaxysE...|\n",
      "|1001087313718296578|  [ChowkidaroKaScam]|\n",
      "|1001099799633022976|[13NovBalochMarty...|\n",
      "|          100136328|[BlacksforTrump20...|\n",
      "|1001619711744987142|      [BTSatWembley]|\n",
      "|1001699058501775362|[ShamInvestigatio...|\n",
      "|1001838583425175552|  [AyudaHumanitaria]|\n",
      "|1001846696958185472|[ebike, CleanAir,...|\n",
      "|1001877127115915264|[MoscowMitch, Mos...|\n",
      "|1001894911283691520|         [Koya, BTS]|\n",
      "+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code here. Explain all steps.\n",
    "data = spark.read.csv(\"s3://us-congress-tweets/user_hashtags.csv\")\n",
    "data = data.select(F.col(\"_c0\").alias(\"id\"), F.col(\"_c1\").alias(\"hashtag\"))\\\n",
    "           .groupby(\"id\").agg(F.collect_list(\"hashtag\").alias(\"hashtags\"))\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.fpm import FPGrowth\n",
    "\n",
    "fpGrowth = FPGrowth(minSupport=0.05, minConfidence=0.05, itemsCol='hashtags')\n",
    "fpGrowthModel = fpGrowth.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+\n",
      "|          items|  freq|\n",
      "+---------------+------+\n",
      "|    [Venezuela]|228140|\n",
      "|  [MoscowMitch]|194200|\n",
      "|[TrumpShutdown]|169301|\n",
      "|[MuellerReport]|164781|\n",
      "+---------------+------+\n",
      "\n",
      "+----------+----------+----------+----+\n",
      "|antecedent|consequent|confidence|lift|\n",
      "+----------+----------+----------+----+\n",
      "+----------+----------+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fpGrowthModel.freqItemsets.show()\n",
    "fpGrowthModel.associationRules.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|               items|  freq|\n",
      "+--------------------+------+\n",
      "|         [Venezuela]|228140|\n",
      "|       [MoscowMitch]|194200|\n",
      "|     [TrumpShutdown]|169301|\n",
      "|[TrumpShutdown, M...| 68271|\n",
      "|     [MuellerReport]|164781|\n",
      "|              [MAGA]|153253|\n",
      "|       [NancyPelosi]|144953|\n",
      "|      [GreenNewDeal]|137423|\n",
      "|         [Kavanaugh]|124906|\n",
      "|          [BREAKING]|112978|\n",
      "|             [Trump]|106536|\n",
      "|              [SOTU]|100974|\n",
      "|           [Mueller]| 90451|\n",
      "|               [HR1]| 88041|\n",
      "|      [ForThePeople]| 87395|\n",
      "|         [Democrats]| 83774|\n",
      "|      [MaduroRegime]| 80650|\n",
      "|[MaduroRegime, Ve...| 68564|\n",
      "|      [BuildTheWall]| 77876|\n",
      "|            [Maduro]| 69136|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------------+---------------+-------------------+------------------+\n",
      "|     antecedent|     consequent|         confidence|              lift|\n",
      "+---------------+---------------+-------------------+------------------+\n",
      "| [MaduroRegime]|    [Venezuela]| 0.8501425914445133|11.456291866552178|\n",
      "|  [MoscowMitch]|[TrumpShutdown]|0.35154994850669413| 6.383829031378436|\n",
      "|    [Venezuela]| [MaduroRegime]|0.30053475935828877|11.456291866552178|\n",
      "|[TrumpShutdown]|  [MoscowMitch]|0.40325219579329125| 6.383829031378436|\n",
      "+---------------+---------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fpGrowth = FPGrowth(minSupport=0.02, minConfidence=0.02, itemsCol='hashtags')\n",
    "fpGrowthModel = fpGrowth.fit(data)\n",
    "fpGrowthModel.freqItemsets.show()\n",
    "fpGrowthModel.associationRules.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+\n",
      "|                 id|            hashtags|          prediction|\n",
      "+-------------------+--------------------+--------------------+\n",
      "|1001877127115915264|[MoscowMich, Leni...|     [TrumpShutdown]|\n",
      "|1002410259447799808|[DNC, RecallFeins...|      [MaduroRegime]|\n",
      "|          100311168|[orteguismo, NICA...|      [MaduroRegime]|\n",
      "|1003808157775245312|[TrumpShutdown, M...|       [MoscowMitch]|\n",
      "|1004865478274244610|[FirstResponders,...|     [TrumpShutdown]|\n",
      "|         1005065702|         [Venezuela]|      [MaduroRegime]|\n",
      "|         1005243704|[MerrickGarland, ...|       [MoscowMitch]|\n",
      "|1005475619566706688|         [Venezuela]|      [MaduroRegime]|\n",
      "|1006736266401144832|[Urgente, Caracas...|      [MaduroRegime]|\n",
      "|1007154784430968834|[NoGlobalism, Men...|     [TrumpShutdown]|\n",
      "|1007386606217637888|[CalDay, NoIranWa...|      [MaduroRegime]|\n",
      "|         1007629891|[TreasonCaucus, J...|[MaduroRegime, Mo...|\n",
      "|         1007762215|       [MoscowMitch]|     [TrumpShutdown]|\n",
      "|1008731226281332736|[HouseJudiciaryCo...|       [MoscowMitch]|\n",
      "|1009058987696967681|[MoscowMitch, Mos...|     [TrumpShutdown]|\n",
      "|1009101039767781378|       [MoscowMitch]|     [TrumpShutdown]|\n",
      "|1009125220228009985|[MoscowMitchMcTre...|     [TrumpShutdown]|\n",
      "|          100973954|[BDS, RussiaInves...|      [MaduroRegime]|\n",
      "|1009786407030845440|[respect, Magat, ...|       [MoscowMitch]|\n",
      "|1009830390767505409|[ForThePeople, Tr...|       [MoscowMitch]|\n",
      "+-------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fpGrowthModel.transform(data).filter(F.size(F.col(\"prediction\")) > 0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List two user communities you think are interesting. Explain why they are reasonable communities.\n",
    "\n",
    "You can use https://twitter.com/intent/user?user_id=? to find out more info about the users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as t\n",
    "\n",
    "def get_community(community_hashtags):\n",
    "    def find_all(user_hashtags):\n",
    "        return community_hashtags.issubset(set(user_hashtags))\n",
    "    \n",
    "    filter_udf = F.udf(find_all, t.BooleanType())\n",
    "    community = data.filter(filter_udf(\"hashtags\"))\n",
    "    \n",
    "    return community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chose the two communities because they frequently appeared in the above association rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|                 id|            hashtags|\n",
      "+-------------------+--------------------+\n",
      "|1002752931341299712|[JETMOMforCongres...|\n",
      "|1006046436272713728|[SundayMotivation...|\n",
      "|1007386606217637888|[Trump, Impeachme...|\n",
      "|         1019032993|[HR8, InherentCon...|\n",
      "|          102259828|[ImpeachTrumpNowA...|\n",
      "|         1028571146|[WorldCup2019, me...|\n",
      "|          102871670|[KochBrothers, Jo...|\n",
      "|1028949579075600384|[SpeakerPelosi, B...|\n",
      "|          103123025|[HR7, opioids, AG...|\n",
      "|1033154968465928192|[MoscowMitchMcTra...|\n",
      "|1035573557546426368|[TrumpShutdown, T...|\n",
      "|1037646284210008064|[RuleOfLaw, 19thA...|\n",
      "|1039926455461916673|[ReleaseTheReport...|\n",
      "|1080427519126528005|[EricGarner, Prot...|\n",
      "|1081583969194442752|[EndCorruptionNow...|\n",
      "|1082472422853496832|[BarrHearing, imp...|\n",
      "|1087237626426359811|[NationalEmergenc...|\n",
      "|         1107755834|[DianneFeinstein,...|\n",
      "|          113182207|[JerryNadler, spe...|\n",
      "|         1151933252|[FreeYoel, FreeMu...|\n",
      "+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# community 1\n",
    "hashtags1 = set([\"MoscowMitch\", \"TrumpShutdown\"])\n",
    "\n",
    "community1 = get_community(hashtags1)\n",
    "community1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------------+\n",
      "|                 id|              hashtags|\n",
      "+-------------------+----------------------+\n",
      "|1007154784430968834|  [BarrHearings, Ju...|\n",
      "|1014566827907788800|  [DaDeLaJuventud,...|\n",
      "|1017582915377561606|  [Venezuela, terri...|\n",
      "|1018743778868383744|  [EnVivo, Rusia, R...|\n",
      "|1024271180826660864|  [Venezuela, Green...|\n",
      "|1032693985301721090|  [Russia, VENEZUEL...|\n",
      "|          103607533|  [Venezuela, Colom...|\n",
      "|1039517587569340416|  [caguaripanolibre...|\n",
      "|1041801626867118080|  [FNS, Venezuelan,...|\n",
      "|          104790247|  [23F, Maduro, SEB...|\n",
      "|          105262982|  [Venezuela, Madur...|\n",
      "|1054504408787177472|  [Venezuela, Madur...|\n",
      "|1055358147576127489|[TrishRegan, , ...|\n",
      "|1057220887022366720|  [Venezuela, 23Feb...|\n",
      "|         1061852376|  [URGENTE, HISTRI...|\n",
      "|         1061869964|  [Retador, 23F, AH...|\n",
      "|1067089074006237185|  [MaduroCrimeFamil...|\n",
      "|         1068787399|  [ConfirmKavanaugh...|\n",
      "|         1071723313|  [UltimaHora, Vene...|\n",
      "|1074965113885593600|  [GNB, FANB, EEUU,...|\n",
      "+-------------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# community 2\n",
    "hashtags2 = set([\"MaduroRegime\", \"Venezuela\"])\n",
    "\n",
    "community2 = get_community(hashtags2)\n",
    "community2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What value for support did you choose and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer here\n",
    "I tried 0.05 and 0.02, and as the results showed, there was not enough interesting information provided by a support of 0.05, which is why I then chose 0.02 as a better support value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (10 points) Part 6: Personalized PageRank\n",
    "Assume you are given a task to recommend Twitter users for the speaker of the House to engage with.\n",
    "\n",
    "Construct a user-mentions network using relations in `s3://us-congress-tweets/user_mentions.csv`\n",
    "\n",
    "Run Personalized PageRank with source (id=15764644) and find out top accounts to recommend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+-----+\n",
      "|               src|               dst|count|\n",
      "+------------------+------------------+-----+\n",
      "|917194889275699201|        1249982359|    1|\n",
      "|917570582555779072|         251918778|    1|\n",
      "|         633674091|         432895323|   48|\n",
      "|913222391416934402|         432895323|  261|\n",
      "|         217574712|          47203904|    1|\n",
      "|931278145679847424|         320757267|    1|\n",
      "|897332217646522368|          92186819|    8|\n",
      "|        2308265716|          18061669|    4|\n",
      "|822826488500088832|        1249982359|  132|\n",
      "|728645535566008320|958064770019741696|    1|\n",
      "|827323185058033665|          14247236|   11|\n",
      "|          36566383|826629809954553856|    2|\n",
      "|913125313172918272|         432895323|   90|\n",
      "|860643294367318016|         432895323|   52|\n",
      "|866790059802079234|         432895323|  148|\n",
      "|888055635828502530|          16056306|    2|\n",
      "|        3141820397|         112006107|    1|\n",
      "|836681162638532608|         262756641|    1|\n",
      "|         157952243|         102391944|    1|\n",
      "|         334871740|          16117316|   17|\n",
      "+------------------+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------------------+---------------+\n",
      "|                id|           name|\n",
      "+------------------+---------------+\n",
      "|         776664410|  RepCartwright|\n",
      "|         240363117|   RepTomMarino|\n",
      "|837722935095930883| RepScottTaylor|\n",
      "|        1069124515|     RepLaMalfa|\n",
      "|818460870573441028|  RepTomGarrett|\n",
      "|         163570705|     repcleaver|\n",
      "|          19739126|      GOPLeader|\n",
      "|          33563161| RepJoseSerrano|\n",
      "|        2861616083|USRepGaryPalmer|\n",
      "|        1074518754| SenatorBaldwin|\n",
      "|         305620929|  Call_Me_Dutch|\n",
      "|         381152398| RepTerriSewell|\n",
      "|         834069080| RepDavidRouzer|\n",
      "|         249787913|  SenatorCarper|\n",
      "|         188019606|        Clyburn|\n",
      "|         217543151|SenatorTimScott|\n",
      "|          39249305| USRepMikeDoyle|\n",
      "|          33537967|   amyklobuchar|\n",
      "|         249410485|  SanfordBishop|\n",
      "|          23124635|    TomColeOK04|\n",
      "+------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "('# of edges:', '191640265')\n",
      "('# of vertices:', '577')\n"
     ]
    }
   ],
   "source": [
    "# your network construction code here\n",
    "edges = spark.read.csv(\"s3://us-congress-tweets/user_mentions.csv\", header=True)\n",
    "vertices = spark.read.csv(\"s3://us-congress-tweets/congress_members.csv\", header=True)\n",
    "vertices = vertices.selectExpr(\"userid as id\", \"screen_name as name\")\n",
    "\n",
    "edges.show()\n",
    "vertices.show()\n",
    "print(\"# of edges:\", str(edges.count()))\n",
    "print(\"# of vertices:\", str(vertices.count()))\n",
    "\n",
    "graph = GraphFrame(vertices, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+--------------------+\n",
      "|                id|outDegree|            pagerank|\n",
      "+------------------+---------+--------------------+\n",
      "|          15764644|      342| 0.16019027695661536|\n",
      "|          25073877|      117|0.005388416407924...|\n",
      "|        1249982359|       58|0.004304160304646484|\n",
      "|          15808765|      691|0.003816526962310861|\n",
      "|822215679726100480|       37|0.003261404382909...|\n",
      "|          17494010|      270|0.003066113870118036|\n",
      "|          29450962|       29|0.002780351747696328|\n",
      "|         787373558|      148|0.002642578668230067|\n",
      "|          72198806|      389|0.002578709615593...|\n",
      "|          29501253|       31|0.002499457112435968|\n",
      "|          15745368|      432|0.002462101382807848|\n",
      "|          10615232|        6|0.002417871994118461|\n",
      "|         970207298|      263|0.002337188857503...|\n",
      "|         432895323|      321|0.002314610927849...|\n",
      "|          33537967|      458|0.002236878744709...|\n",
      "|         476256944|      111|0.002183393909708038|\n",
      "|        1880674038|      592|0.002097071794875...|\n",
      "|          43963249|      198|0.002085717997697156|\n",
      "|        2970279814|      330|0.002078746294983...|\n",
      "|          40302336|      354|0.002072357009265...|\n",
      "+------------------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your Personalized PageRank code here\n",
    "graph = GraphFrame(graph.outDegrees, edges)\n",
    "pageranks = graph.pageRank(resetProbability=0.15, maxIter=10, sourceId=\"15764644\")\n",
    "pageranks.vertices.sort(F.desc(\"pagerank\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 accounts to recommend \n",
    "# You can use https://twitter.com/intent/user?user_id=? to find out more info about the users\n",
    "Nancy Pelosi @SpeakerPelosi 15764644\n",
    "Donald J. @realDonaldTrump 25073877\n",
    "Leader McConnell @senatemajldr 1249982359\n",
    "Gory BOOker  @CoryBooker 15808765\n",
    "President Trump @POTUS 822215679726100480\n",
    "Chuck Schumer @SenSchumer 17494010\n",
    "John Lewis @repjohnlewis 29450962\n",
    "Elijah E. Cummings @RepCummings 787373558\n",
    "Kirsten Gillibrand @SenGillibrand 72198806\n",
    "Adam Schiff @RepAdamSchiff 29501253"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Troubleshooting Tips\n",
    "\n",
    "* If you get \"spark not available\" error, this most likely means the Kernel is python and not PySpark. Just change the Kernel to PySpark and it should work.\n",
    "\n",
    "\n",
    "* If your notebook seems stuck (may happen if you force stop a cell), you may need to ssh to your master node and kill the spark application associated with the notebook     \n",
    "    Use `yarn application -list` to find the application id and then `yarn application -kill app-id` to kill it. After that restart your notebook from the browser.\n",
    "\n",
    "\n",
    "* If you like, you may also ssh to the master node and run `pyspark` and execute your code directly in the shell.\n",
    "\n",
    "* If you face difficulties accessing the pages for the jobs for example to see logs and so on then you can open all needed ports when you create the cluster. (e.g. 8088)\n",
    "\n",
    "* If you want to see logs for a MapReduce job from the terminal use the following command:\n",
    "\n",
    "    `yarn logs -applicationId <application_id>`\n",
    "\n",
    "\n",
    "* To kill a MapReduce job use:\n",
    "\n",
    "    `yarn  application -kill <application_id>`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
